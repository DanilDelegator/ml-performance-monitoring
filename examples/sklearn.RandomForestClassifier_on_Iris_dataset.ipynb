{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5096ee85-a0d8-40aa-abf1-406ccee32daf",
   "metadata": {},
   "source": [
    "# New Relic ML Performance Monitoring- Bring Your Own Data\n",
    "\n",
    "[ml-performance-monitoring](https://github.com/newrelic-experimental/ml-performance-monitoringg) provides a Python library for sending machine learning models' inference data and performance metrics into New Relic. \n",
    "<br>\n",
    "It is based on the [newrelic-telemetry-sdk-python](https://github.com/newrelic/newrelic-telemetry-sdk-python) library.\n",
    "<br>\n",
    "By using this package, you can easily and quickly monitor your model, directly from a Jupyter notebook or a cloud service. \n",
    " <br>\n",
    "This notebook provides an example of sending inference data and metrics of an RandomForestClassifier model\n",
    "\n",
    "Note:\n",
    "This notebook uses the libraries: numpy, pandas, sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf23f3-4ab0-4e52-9c3a-5c150590cc39",
   "metadata": {},
   "source": [
    "### 1. Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ee9c03-5043-4a26-94e6-5924cf549cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from ml_performance_monitoring.monitor import MLPerformanceMonitoring, wrap_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432798ef-1fb1-4006-9510-84c4778454a9",
   "metadata": {},
   "source": [
    "### 2. Load the Iris dataset and split it into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "302cf098-c363-4989-90a7-1947340eebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2]]),\n",
       " array([0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "X, y = (\n",
    "    iris_dataset[\"data\"],\n",
    "    iris_dataset[\"target\"],\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")\n",
    "\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212217d-1573-4ccd-98e0-3725800d8274",
   "metadata": {},
   "source": [
    "### 3. Fitting Random Forest Classification model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb62a1bb-28ea-4105-93be-62d0a4c8deaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection',\n",
       "                 SelectKBest(k=2, score_func=<function chi2 at 0x13d02e040>)),\n",
       "                ('classification', RandomForestClassifier())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set up a pipeline with a feature selection preprocessor that\n",
    "# selects the top 2 features to use.\n",
    "# The pipeline then uses a RandomForestClassifier to train the model.\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"feature_selection\", SelectKBest(chi2, k=2)),\n",
    "        (\"classification\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5c717-58d3-4b41-88c3-549a044d7d7d",
   "metadata": {},
   "source": [
    "### 4. Predicting the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcced287-1691-4fd3-a802-bea65f5edf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 1, 0, 0, 1,\n",
       "       0, 2, 0, 0, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66de2b-e106-4a81-a5f8-83fb252e55fd",
   "metadata": {},
   "source": [
    "### 5. Record inference data to New Relic\n",
    "<b> You have two options for sending your model's inference data (features and predictions) to New Relic: </b>\n",
    "<br> 1. \"Online\" instrumentation - sending the data while the model is being invoked in production\n",
    "<br> 2. \"Offline\" instrumentation - sending the data as a dataset (as an np.array or pandas dataframe) \n",
    "\n",
    "<b>   The MLPerformanceMonitoring object requires the following parameters: </b>\n",
    "<br> 1. <b>model_name</b> - must be unique per model\n",
    "<br> 2. <b>New Relic insert key </b>-  can be license-key or insights-insert-key [how to get your insert key](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#insights-insert-key), set it as the following environment variable: NEW_RELIC_INSERT_KEY <br>\n",
    "Optional parameters:\n",
    "<br> \n",
    "3. <b>metadata</b> (dictionary) - will be added to each event (row) of the data \n",
    "<br>\n",
    "4. <b>send_data_metrics </b>(boolean) - send data metrics (statistics) to New Relic (False as default)\n",
    "<br> \n",
    "5. <b>features_columns</b> (list) - the features' names ordered as X columns. On New Relic data, the names will be prefixed with the string 'feature_'\n",
    "<br>\n",
    "6. <b> labels_columns</b> (list) - the labels' names ordered as y columns. On New Relic data, the names will be prefixed with the string 'label_'\n",
    "<br> (note: The parameters features_columns and labels_columns are only relevant when sending the data as an np.array. When the data is sent as a dataframe, the dataframes (X,y) columns' names will be taken as features and labels names respectively. In addition, if you send your data as an np.array without sending the features_columns and labels_columns, on New Relic data, the names will appear as \"feature_{n}\" and \"lablel_{n}\" numbered by the features/labels order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b74dc293-03cb-4e05-99c2-fc88a3c513a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\"environment\": \"aws\", \"dataset\": \"Iris\", \"version\": \"1.0\"}\n",
    "features_columns = (\n",
    "    [\n",
    "        \"sepal_length\",\n",
    "        \"sepal_width\",\n",
    "        \"petal_length\",\n",
    "        \"petal_width\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "labels_columns = [\"species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ee01e-c2ad-4c38-a870-6017d065a7e6",
   "metadata": {},
   "source": [
    "<b> 5.1.  \"Online\" instrumentation </b>\n",
    "<br>\n",
    "The wrap_model function extends the model/pipeline methods with the functionality of sending the inference data as [custom event](https://docs.newrelic.com/docs/data-apis/ingest-apis/introduction-event-api/) named \"InferenceData\" to New Relic NRDB.\n",
    "Wrap your model or pipeline by sending it as a parameter and then use it (the return value) as usual (fit, predict, etc.). Your inference data and data metrics will be sent automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f58174-be3d-4801-861f-1b53797f413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference data sent successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/vy1ftggn3kdfnd9cjmwt_nc40000gn/T/ipykernel_26654/1779865758.py:228: UserWarning: send_data_metrics occurs only when there are at least 100 rows\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ml_performence_monitor_pipeline = wrap_model(\n",
    "    insert_key=None,  # set the environment variable NEW_RELIC_INSERT_KEY or send your insert key here\n",
    "    model=pipeline,\n",
    "    model_name=\"RandomForestClassifier on Iris Dataset\",\n",
    "    metadata=metadata,\n",
    "    send_data_metrics=True,\n",
    "    features_columns=features_columns,\n",
    "    labels_columns=labels_columns,\n",
    ")\n",
    "\n",
    "y_pred = ml_performence_monitor_pipeline.predict(\n",
    "    X=X_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33a366-ab59-4b87-9a2e-0900cfc3a1cf",
   "metadata": {},
   "source": [
    "<b> 5.2.  \"Offline\" instrumentation </b>\n",
    "<br>\n",
    "Define an MLPerformanceMonitoring object and send your inference data and data metrics as an np.array or as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8638b81c-a85b-4901-a7ec-9f46b84dd259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/vy1ftggn3kdfnd9cjmwt_nc40000gn/T/ipykernel_26654/1779865758.py:47: UserWarning: model wasn't defined, please use 'record_inference_data' to send data\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ml_monitor = MLPerformanceMonitoring(\n",
    "    insert_key=None,  # set the environment variable NEW_RELIC_INSERT_KEY or send your insert key here,\n",
    "    model_name=\"RandomForestClassifier on Iris Dataset1\",\n",
    "    metadata=metadata,\n",
    "    send_data_metrics=True,\n",
    "    features_columns=features_columns,\n",
    "    labels_columns=labels_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6f13d-83f4-43e3-ab8c-8f027e4fa723",
   "metadata": {},
   "source": [
    "<b> 5.2.1  Send your features and predictions as an np.array. </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4741c115-f94f-4867-9249-17fc39119089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference data sent successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/vy1ftggn3kdfnd9cjmwt_nc40000gn/T/ipykernel_26654/1779865758.py:228: UserWarning: send_data_metrics occurs only when there are at least 100 rows\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ml_monitor.record_inference_data(X=X_test, y=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07124c7-98b6-45fc-83c8-ae8a1d7d7ee8",
   "metadata": {},
   "source": [
    "<b> 5.2.2  Send your features and prediction as a pd.DataFrame. <br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a8a252-675f-4f17-a325-6c940a0ad4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                6.3               2.5                4.9               1.5\n",
       "1                6.8               3.0                5.5               2.1\n",
       "2                6.4               2.8                5.6               2.2\n",
       "3                5.6               3.0                4.1               1.3\n",
       "4                4.9               3.6                1.4               0.1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = pd.DataFrame(\n",
    "    list(map(np.ravel, X_test)),\n",
    "    columns=features_columns,\n",
    ")\n",
    "\n",
    "y_pred_df = pd.DataFrame(\n",
    "    list(map(np.ravel, y_pred)),\n",
    "    columns=labels_columns,\n",
    ")\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8f58775-0e59-4689-8c9c-48a8704ad0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0       1\n",
       "1       2\n",
       "2       2\n",
       "3       1\n",
       "4       0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cf674cc-835e-4ea1-9402-20fd689b2096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference data sent successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/vy1ftggn3kdfnd9cjmwt_nc40000gn/T/ipykernel_26654/1779865758.py:228: UserWarning: send_data_metrics occurs only when there are at least 100 rows\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ml_monitor.record_inference_data(X=X_df, y=y_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd79a4-335e-46f1-b28b-b0e4da4081ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Record metrics to New Relic\n",
    "You can stream custom metrics to New Relic, monitoring your model performance or model data. These metrics will be sent to NRDB as [metric data](https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/introduction-metric-api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc20d627-9357-498f-a609-81636bb20358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    : 0.9333333333333333\n",
      "Recall      : 0.9333333333333333\n",
      "Precision   : 0.95\n",
      "F1 Score    : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# Model Evaluation\n",
    "ac_sc = accuracy_score(y_test, y_pred)\n",
    "rc_sc = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "pr_sc = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "f1_sc = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Accuracy    : {ac_sc}\")\n",
    "print(f\"Recall      : {rc_sc}\")\n",
    "print(f\"Precision   : {pr_sc}\")\n",
    "print(f\"F1 Score    : {f1_sc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36710290-5eae-4bc3-b04b-f77a26c053ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_metric sent successfully\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"Accuracy\": ac_sc,\n",
    "    \"Recall\": rc_sc,\n",
    "    \"Precision\": pr_sc,\n",
    "    \"F1_Score\": f1_sc,\n",
    "}\n",
    "metrics\n",
    "ml_monitor.record_metrics(metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f3475-dba8-4529-8de5-f770d3d44924",
   "metadata": {},
   "source": [
    "### 7. Optional - Simulate 24 hours of model inference data  \n",
    "As written, the main purpose of this library is to record inference data of a scheduled model in production. By running the cell below, a simulation of inference data of the RandomForest Classifier model on the Iris Dataset will run each hour in the last 24 hours. After running the cell, a relevant dashboard will automatically be built in this link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c8657-b4b4-4d49-9582-248d5a1673a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=10, criterion=\"entropy\", random_state=0\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "ml_monitor = MLPerformanceMonitoring(\n",
    "    insert_key=None,  # set the environment variable NEW_RELIC_INSERT_KEY or send your insert key here,\n",
    "    model_name=\"RandomForest Classifier on Iris Dataset - Inference Simulation\",\n",
    "    metadata=metadata,\n",
    "    features_columns=features_columns,\n",
    "    labels_columns=labels_columns,\n",
    ")\n",
    "\n",
    "last_24h_date = lastHourDateTime = datetime.now() - timedelta(hours=24)\n",
    "last_24h_timestamp = int(datetime.timestamp(last_24h_date) * 1000)\n",
    "\n",
    "for i in range(24):\n",
    "\n",
    "    idx = np.random.choice(np.arange(len(X)), 10, replace=False)\n",
    "    X_sample = X[idx]\n",
    "    y_sample = y[idx]\n",
    "\n",
    "    y_pred = classifier.predict(X_sample)\n",
    "\n",
    "    X_df = pd.DataFrame(\n",
    "        list(map(np.ravel, X_sample)),\n",
    "        columns=features_columns,\n",
    "    )\n",
    "\n",
    "    y_pred_df = pd.DataFrame(\n",
    "        list(map(np.ravel, y_pred)),\n",
    "        columns=labels_columns,\n",
    "    )\n",
    "\n",
    "    y_pred_df.loc[y_pred_df.species == 0, \"species\"] = \"Setosa\"\n",
    "    y_pred_df.loc[y_pred_df.species == 1, \"species\"] = \"Versicolour\"\n",
    "    y_pred_df.loc[y_pred_df.species == 2, \"species\"] = \"Virginica\"\n",
    "\n",
    "    ml_monitor.record_inference_data(\n",
    "        X=X_df, y=y_pred_df, timestamp=last_24h_timestamp + i * 3600000\n",
    "    )\n",
    "\n",
    "    # Model Evaluation\n",
    "    ac_sc = accuracy_score(y_sample, y_pred)\n",
    "    rc_sc = recall_score(y_sample, y_pred, average=\"weighted\")\n",
    "    pr_sc = precision_score(y_sample, y_pred, average=\"weighted\")\n",
    "    f1_sc = f1_score(y_sample, y_pred, average=\"micro\")\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": ac_sc,\n",
    "        \"Recall\": rc_sc,\n",
    "        \"Precision\": pr_sc,\n",
    "        \"F1_Score\": f1_sc,\n",
    "    }\n",
    "    metrics\n",
    "    ml_monitor.record_metrics(metrics=metrics)\n",
    "\n",
    "    X_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
